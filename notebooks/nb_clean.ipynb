{
 "cells": [
  {
   "cell_type": "code",
   "id": "f95b1f39",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "20ae2546",
   "metadata": {},
   "source": [
    "# Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd03e7d3",
   "metadata": {},
   "source": [
    "data_path = \"../data/welddb.data\"\n",
    "column_names = [\n",
    "    'C', 'Si', 'Mn', 'S', 'P', 'Ni', 'Cr', 'Mo', 'V', 'Cu', 'Co', 'W', \n",
    "    'O', 'Ti', 'N', 'Al', 'B', 'Nb', 'Sn', 'As', 'Sb', \n",
    "    'Current', 'Voltage', 'AC_DC', 'Electrode_polarity', 'Heat_input', 'Interpass_temp',  \n",
    "    'Weld_type', 'PWHT_temp', 'PWHT_time', \n",
    "    'Yield_strength', 'UTS', 'Elongation', 'Reduction_area',  \n",
    "    'Charpy_temp', 'Charpy_impact', 'Hardness', 'FATT_50', \n",
    "    'Primary_ferrite', 'Ferrite_2nd_phase', 'Acicular_ferrite', 'Martensite', 'Ferrite_carbide', \n",
    "    'Weld_ID' \n",
    "]\n",
    "df = pd.read_csv(data_path, sep='\\s+', names=column_names, na_values='N') # as the NaN values are represented by 'N' in the data file\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "de0a5e01",
   "metadata": {},
   "source": [
    "categorical_cols = ['AC_DC', 'Electrode_polarity', 'Weld_type', 'Weld_ID']\n",
    "chemical_cols = ['C','Si','Mn','S','P','Ni','Cr','Mo','V','Cu','Co',\n",
    "                 'W','O','Ti','N','Al','B','Nb','Sn','As','Sb']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5ce64a65",
   "metadata": {},
   "source": [
    "# Data statistics"
   ]
  },
  {
   "cell_type": "code",
   "id": "e1c88346",
   "metadata": {},
   "source": [
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "130780c2",
   "metadata": {},
   "source": [
    "On remarque que beaucoup de colonnes ont une majorité de valeurs manquantes. "
   ]
  },
  {
   "cell_type": "code",
   "id": "2e29afcc",
   "metadata": {},
   "source": [
    "print(f\"Proportion of missing values: {df.isna().sum().sum() / df.size}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6c9e69cd",
   "metadata": {},
   "source": [
    "df_missing = df.isna().sum().sort_values(ascending=False) / len(df)\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=df_missing.index, y=df_missing.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Proportion of missing values')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec9f7cc8",
   "metadata": {},
   "source": [
    "df_missing_non_chemical = df_missing.drop(chemical_cols)\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=df_missing_non_chemical.index, y=df_missing_non_chemical.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Proportion of missing values')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2b23090",
   "metadata": {},
   "source": [
    "df_missing_non_chemical * len(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2aea3656",
   "metadata": {},
   "source": [
    "# Visualize missing data pattern\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.isna(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Data Pattern')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f05ac80",
   "metadata": {},
   "source": [
    "# visualize for non-chemical features only\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.drop(columns=chemical_cols).isna(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Data Pattern (Non-Chemical Features)')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c60b2eec",
   "metadata": {},
   "source": [
    "On a des colonnes qui ont l'air d'être toujours présentes ensemble et d'autres alors que les autres sont absentes. Comme si l'on avait fusionner des datasets différents sur des colonnes en commun. \n",
    "\n",
    "Les colonnes communes seraient les colonnes des éléments chimiques, et les colonnes : [\"Current\", \"Voltage\", \"AC_DC\", \"Electrode_polarity\", \"Heat_input\", \"Interpass_temp\", \"Weld_type\", \"PWHT_temp\", \"PWHT_time\"].\n",
    "\n",
    "Les colonnes du premier dataset : [\"Yield_strength\", \"UTS\", \"Elongation\", \"Reduction_area\"]\n",
    "\n",
    "Deuxième : [\"Charpy_temp\", \"Charpy_impact\"]\n",
    "\n",
    "Troisième : [\"Primary_ferrite\", \"Ferrite_2nd_phase\", \"Acicular_ferrite\", \"Martensite\", \"Ferrite_carbide\"]\n",
    "\n",
    "Hardness et FATT_50 ont l'air seuls. "
   ]
  },
  {
   "cell_type": "code",
   "id": "ea60e8be",
   "metadata": {},
   "source": [
    "common_columns = chemical_cols + [\"Current\", \"Voltage\", \"AC_DC\", \"Electrode_polarity\", \"Heat_input\", \"Interpass_temp\", \"Weld_type\", \"PWHT_temp\", \"PWHT_time\"]\n",
    "first_dataset_cols = [\"Yield_strength\", \"UTS\", \"Elongation\", \"Reduction_area\"]\n",
    "second_dataset_cols = [\"Charpy_temp\", \"Charpy_impact\"]\n",
    "third_dataset_cols = [\"Primary_ferrite\", \"Ferrite_2nd_phase\", \"Acicular_ferrite\", \"Martensite\", \"Ferrite_carbide\"]\n",
    "\n",
    "df_first = df[common_columns + first_dataset_cols]\n",
    "df_second = df[common_columns + second_dataset_cols]\n",
    "df_third = df[common_columns + third_dataset_cols]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2a0cd4d0",
   "metadata": {},
   "source": [
    "idx_1 = set(list(df_first.dropna(subset=first_dataset_cols, how='all').index))\n",
    "idx_2 = set(list(df_second.dropna(subset=second_dataset_cols, how='all').index))\n",
    "idx_3 = set(list(df_third.dropna(subset=third_dataset_cols, how='all').index))\n",
    "\n",
    "print(f\"intersection of all three datasets: {idx_1 & idx_2 & idx_3}\")\n",
    "print(f\"intersection of first and second datasets: {idx_1 & idx_2}\")\n",
    "print(f\"intersection of first and third datasets: {idx_1 & idx_3}\")\n",
    "print(f\"intersection of second and third datasets: {idx_2 & idx_3}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "33f891ba",
   "metadata": {},
   "source": [
    "print(f\"Number of samples in intersection of first and second datasets: {len(idx_1 & idx_2)}\")\n",
    "print(f\"Number of samples in intersection of first and third datasets: {len(idx_1 & idx_3)}\")\n",
    "print(f\"Number of samples in intersection of second and third datasets: {len(idx_2 & idx_3)}\")\n",
    "print(\"\\n\"+\"_\"*90+\"\\n\")\n",
    "print(f\"Part de valeurs du dataset 2 présentes dans le dataset 1: {len(idx_1 & idx_2)/len(idx_1):.2%}\")\n",
    "print(f\"Part de valeurs du dataset 1 présentes dans le dataset 2: {len(idx_1 & idx_2)/len(idx_2):.2%}\")\n",
    "print(f\"Part de valeurs du dataset 3 présentes dans le dataset 1: {len(idx_1 & idx_3)/len(idx_1):.2%}\")\n",
    "print(f\"Part de valeurs du dataset 1 présentes dans le dataset 3: {len(idx_1 & idx_3)/len(idx_3):.2%}\")\n",
    "print(f\"Part de valeurs du dataset 3 présentes dans le dataset 2: {len(idx_2 & idx_3)/len(idx_2):.2%}\")\n",
    "print(f\"Part de valeurs du dataset 2 présentes dans le dataset 3: {len(idx_2 & idx_3)/len(idx_3):.2%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1af58eb4",
   "metadata": {},
   "source": [
    "len(idx_1), len(idx_2), len(idx_3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4e00b955",
   "metadata": {},
   "source": [
    "Le dataset 1 et 3 sont complètements indépendants. Le dataset 2 n'a presque pas de valeurs en commun avec le dataset 3, on les considère indépendant aussi. Le dataset 3 n'a que très peu de valeur (98) et est à moitié composé du dataset 2. Pour essayer de faire de l'imputing on a trop peu de valeurs, on drop ces colonnes. \n",
    "\n",
    "Le dataset 1 et 2 ont respectivement 17.73% et 16.38% de valeurs communes et composent chacun environ la moitié du dataset. On va pouvoir utiliser ces deux datasets.  "
   ]
  },
  {
   "cell_type": "code",
   "id": "5e9312de",
   "metadata": {},
   "source": [
    "(df['Hardness'].isna() == False).sum(), (df['FATT_50'].isna() == False).sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43a3ed82",
   "metadata": {},
   "source": [
    "Hardness et FATT_50 ont toutes 2 trop peu de valeurs, on va les drop. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ce8a3",
   "metadata": {},
   "source": [
    "# Gestion des colonnes communes "
   ]
  },
  {
   "cell_type": "code",
   "id": "c23cdd30",
   "metadata": {},
   "source": [
    "df_common = df[common_columns]\n",
    "df_common"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "635c8b9f",
   "metadata": {},
   "source": [
    "non_numeric_cols = []\n",
    "for dtype in df_common.dtypes.unique():\n",
    "    if dtype == 'object':\n",
    "        non_numeric_cols.extend(df_common.columns[df_common.dtypes == dtype].tolist())\n",
    "non_numeric_cols\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa930ab9",
   "metadata": {},
   "source": [
    "## Gestion des colonnes chimiques non-numériques"
   ]
  },
  {
   "cell_type": "code",
   "id": "d8043cd0",
   "metadata": {},
   "source": [
    "non_num_chemical_cols = ['S','Mo','V','Cu','Co','W','Ti','N','Al','B','Nb','Sn','As','Sb']\n",
    "\n",
    "# Lorsque nan on remplace par 0 pour les colonnes chimiques \n",
    "df_common.fillna({col: 0 for col in chemical_cols}, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1be4a9b4",
   "metadata": {},
   "source": [
    "# On regarde les valeurs uniques pour chaque colonne chimique non-numérique\n",
    "\n",
    "non_num_vals = {}\n",
    "for col in non_num_chemical_cols:\n",
    "    unique_val_col = df_common[col].dropna().unique()\n",
    "    for val in unique_val_col:\n",
    "        try:\n",
    "            float(val)\n",
    "        except:\n",
    "            non_num_vals[col] = non_num_vals.get(col, []) + [val]\n",
    "non_num_vals"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "59b51f07",
   "metadata": {},
   "source": [
    "for col in non_num_chemical_cols:\n",
    "    tmp = df[col].astype(float, errors='ignore')\n",
    "    tmp.replace(non_num_vals.get(col, []), np.nan, inplace=True)\n",
    "    tmp.dropna(inplace=True)\n",
    "    tmp = tmp.astype(float)\n",
    "    tmp = tmp.describe()\n",
    "    print(f\"{col}:\\n{tmp}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "769521e3",
   "metadata": {},
   "source": [
    "Les unités sont différentes entre les colonnes. On va normaliser après avoir remplacer les valeurs. Pour la colonne 'N' on peut prendre le premier nombre qui a l'air d'être le total (tot). "
   ]
  },
  {
   "cell_type": "code",
   "id": "dc4afd5a",
   "metadata": {},
   "source": [
    "df_common.loc[:, 'N'] = df_common['N'].apply(lambda x: float(str(x).split('tot')[0]) if 'tot' in str(x) else x).astype(float)\n",
    "df_common[\"N\"] = df_common[\"N\"].astype(float)\n",
    "df_common[\"N\"].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c31ea729",
   "metadata": {},
   "source": [
    "Par simplicité, on va remplacer les valeurs du type \"< x\" par r*x avec r un nombre aléatoire entre 0.5 et 1."
   ]
  },
  {
   "cell_type": "code",
   "id": "3a3b35ac",
   "metadata": {},
   "source": [
    "for col in non_num_chemical_cols:\n",
    "    tmp = df[col].astype(float, errors='ignore')\n",
    "    tmp.replace(non_num_vals.get(col, []), np.nan, inplace=True)\n",
    "    tmp.dropna(inplace=True)\n",
    "    tmp = tmp.astype(float)\n",
    "    \n",
    "    std = tmp.std()\n",
    "    df_common.loc[:, col] = df_common[col].apply(lambda x: float(x.split('<')[1]) * np.random.uniform(0.5,1) if '<' in str(x) else x).astype(float)\n",
    "    df_common[col] = df_common[col].astype(float)\n",
    "\n",
    "df_common[non_num_chemical_cols].describe()\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6b0e768f",
   "metadata": {},
   "source": [
    "## Gestion des colonnes non chimiques catégorielles."
   ]
  },
  {
   "cell_type": "code",
   "id": "6dcf7881",
   "metadata": {},
   "source": [
    "no_chem_common_categorical_col = [ 'AC_DC','Electrode_polarity', 'Interpass_temp','Weld_type']\n",
    "\n",
    "df_common[no_chem_common_categorical_col].info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b2ae5c6",
   "metadata": {},
   "source": [
    "Interpass_temp et Weld_type n'ont pas de nan. On commence par gérer celles-ci."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fdc71b",
   "metadata": {},
   "source": [
    "### Gestion des colonnes Interpass_temp et Weld_type"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5cdaf77",
   "metadata": {},
   "source": [
    "df_common[\"Weld_type\"].unique() # Colonne purement catégorielle, on peut faire du one-hot encoding"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ded4bffc",
   "metadata": {},
   "source": [
    "df_common = pd.get_dummies(df_common, columns=['Weld_type'], prefix='Weld_type', drop_first=False, dtype=int)\n",
    "df_common"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2770896a",
   "metadata": {},
   "source": [
    "df_common[\"Interpass_temp\"].unique() # On a que des valeurs numériques ou des intervalles du type x0-x1. \n",
    "# On va remplacer les intervalles par leur moyenne. Un nombre aléatoire entre les deux bornes centré sur la moyenne."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d09027b",
   "metadata": {},
   "source": [
    "def replace_interval(val):\n",
    "    if '-' in str(val):\n",
    "        bounds = val.split('-')\n",
    "        lower_bound = float(bounds[0])\n",
    "        upper_bound = float(bounds[1])\n",
    "        to_ret = np.random.normal(loc=(lower_bound + upper_bound) / 2, scale=(upper_bound - lower_bound) / 7)\n",
    "        if to_ret < lower_bound:\n",
    "            to_ret = lower_bound\n",
    "        if to_ret > upper_bound:\n",
    "            to_ret = upper_bound\n",
    "        return to_ret\n",
    "    else:\n",
    "        return float(val)\n",
    "    \n",
    "test =[replace_interval(\"100-200\") for i in range(100000)]\n",
    "plt.hist(test, bins=100)\n",
    "plt.title(\"Distribution des valeurs remplaçant l'intervalle 100-200 (exemple)\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "441908e2",
   "metadata": {},
   "source": [
    "df_common.loc[:, \"Interpass_temp\"] = df_common[\"Interpass_temp\"].apply(replace_interval).astype(float)\n",
    "df_common[\"Interpass_temp\"] = df_common[\"Interpass_temp\"].astype(float)\n",
    "df_common[\"Interpass_temp\"].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "abb50eaa",
   "metadata": {},
   "source": [
    "### Gestion des colonnes AC_DC et Electrode_polarity "
   ]
  },
  {
   "cell_type": "code",
   "id": "6f2b3d84",
   "metadata": {},
   "source": [
    "df_common['AC_DC'].unique(), df_common['Electrode_polarity'].unique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb5b79c6",
   "metadata": {},
   "source": [
    "sns.heatmap(df_common[['AC_DC', 'Electrode_polarity']].isna())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a30d307",
   "metadata": {},
   "source": [
    "df_common[['AC_DC', 'Electrode_polarity']].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ea627ee3",
   "metadata": {},
   "source": [
    "Les colonnes sont quasiment constantes : DC apparaît 97% du temps et + apparaît 96% du temps. \n",
    "\n",
    "On va tester un imputing avec des modèles robustes au problème de déséquilibre de classe (Arbres)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485598fc",
   "metadata": {},
   "source": [
    "#### Imputing AC_DC "
   ]
  },
  {
   "cell_type": "code",
   "id": "fbcb3a65",
   "metadata": {},
   "source": [
    "# import Kfold, RandomForestClassifier, XGBoostClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Pas besoin de scaler les données pour les modèles d'arbres\n",
    "\n",
    "full = df_common[list(set(df_common.columns) - set(['Electrode_polarity']))].copy()\n",
    "full.dropna(inplace=True)\n",
    "\n",
    "cols = list(set(full.columns) - set(['AC_DC', 'Electrode_polarity', 'Current', 'Voltage'])) # On enlève Current et Voltage car nan\n",
    "\n",
    "X = full[cols]\n",
    "y = full['AC_DC']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6c2ba657",
   "metadata": {},
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "model = AdaBoostClassifier(n_estimators=100, random_state=42, algorithm='SAMME')\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    print((model.predict(X_test) == y_test).sum(), \"predictions correctes sur\", len(y_test))\n",
    "    print(f\"Fold accuracy: {score:.2%}\")\n",
    "    # matric confusionn\n",
    "    print(confusion_matrix(y_test, model.predict(X_test)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "56b8aaaa",
   "metadata": {},
   "source": [
    "On a de très bons résultats avec le AdaBoostClassifier, on peut facilement remplacer les valeurs manquantes. \n",
    "\n",
    "Résultat peut être trop bon --> voir s'il n'y a pas une merde mais ça a l'air ok"
   ]
  },
  {
   "cell_type": "code",
   "id": "58c98346",
   "metadata": {},
   "source": [
    "# Imputing AC_DC\n",
    "model.fit(X, y)\n",
    "df_common.loc[df_common['AC_DC'].isna(), 'AC_DC'] = model.predict(df_common.loc[df_common['AC_DC'].isna(), cols])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b7e60802",
   "metadata": {},
   "source": [
    "#### Imputing Electrode_polarity"
   ]
  },
  {
   "cell_type": "code",
   "id": "4248201a",
   "metadata": {},
   "source": [
    "full = df_common[list(set(df_common.columns) - set(['AC_DC']))].copy()\n",
    "full.dropna(inplace=True)\n",
    "\n",
    "cols = list(set(full.columns) - set(['AC_DC', 'Electrode_polarity', 'Current', 'Voltage'])) # On enlève Current et Voltage car nan\n",
    "\n",
    "X = full[cols]\n",
    "y = full['Electrode_polarity']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e814e918",
   "metadata": {},
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "model = AdaBoostClassifier(n_estimators=100, random_state=42, algorithm='SAMME')\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    print((model.predict(X_test) == y_test).sum(), \"predictions correctes sur\", len(y_test))\n",
    "    print(f\"Fold accuracy: {score:.2%}\")\n",
    "    # matric confusionn\n",
    "    print(confusion_matrix(y_test, model.predict(X_test)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "77ac7690",
   "metadata": {},
   "source": [
    "Ici aussi les résultats ont l'air très bon. Les modèles n'ignorent pas une des classe. On peut l'utiliser."
   ]
  },
  {
   "cell_type": "code",
   "id": "561ed245",
   "metadata": {},
   "source": [
    "# Imputing Electrode_polarity\n",
    "model.fit(X, y)\n",
    "df_common.loc[df_common['Electrode_polarity'].isna(), 'Electrode_polarity'] = model.predict(df_common.loc[df_common['Electrode_polarity'].isna(), cols])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4c25288",
   "metadata": {},
   "source": [
    "# AC_DC est catégorielle, on fait du one-hot encoding\n",
    "df_common = pd.get_dummies(df_common, columns=['AC_DC'], prefix='AC_DC', drop_first=False, dtype=int)\n",
    "\n",
    "# Electrode_polarity est catégorielle, on fait du one-hot encoding\n",
    "df_common = pd.get_dummies(df_common, columns=['Electrode_polarity'], prefix='Electrode_polarity', drop_first=False, dtype=int)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ccb0572",
   "metadata": {},
   "source": [
    "#### Gestion des nan dans les colonnes numériques"
   ]
  },
  {
   "cell_type": "code",
   "id": "ed323731",
   "metadata": {},
   "source": [
    "df_common.isna().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c69db7f4",
   "metadata": {},
   "source": [
    "cols_na = ['Current', 'Voltage', 'PWHT_temp', 'PWHT_time']\n",
    "\n",
    "df_common[cols_na].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bbc311ff",
   "metadata": {},
   "source": [
    "df_common[cols_na]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32e1c6aa",
   "metadata": {},
   "source": [
    "# Plot of the distributions of the columns with missing values\n",
    "for col in cols_na:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.histplot(df_common[col].dropna(), bins=50, kde=True)\n",
    "    plt.title(f'Distribution of {col} (non-missing values)')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "956654f4",
   "metadata": {},
   "source": [
    "Les distributions sont très disparates. On va essayer de remplacer les valeurs manquantes avec les mêmes méthodes que précédemment."
   ]
  },
  {
   "cell_type": "code",
   "id": "2a5b19ab",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "models = {}\n",
    "\n",
    "for col in cols_na:\n",
    "    full = df_common[list(set(df_common.columns) - (set(cols_na) - {col}))].copy()\n",
    "    full.dropna(inplace=True)\n",
    "\n",
    "    cols = list(set(full.columns) - set(cols_na)) \n",
    "\n",
    "    X = full[cols]\n",
    "    y = full[col]\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    model = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        #score = model.score(X_test, y_test)\n",
    "        score = np.sqrt(np.mean((model.predict(X_test) - y_test) ** 2))\n",
    "        print(f\"Fold RMSE for {col}: {score}\")\n",
    "        print(f\"Fold R2 for {col}: {model.score(X_test, y_test):.2%}\\n\")\n",
    "    \n",
    "    print(\"_\"*40+\"\\n\")\n",
    "    model.fit(X, y)\n",
    "    models[col] = (model, cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8288266",
   "metadata": {},
   "source": [
    "Pour les variables Current et Voltage, le modèle est performant donc on va pouvoir compléter les valeurs manquantes. Pour les colonnes PWHT_temp et PWHT_time les résultats sont bien moins bon et les résultats varient beaucoup d'un plie à l'autre, montrant une grande variance dans les résultats. Etant donné qu'il n'y a que peu de valeurs manquantes (13) on va tout de même les compléter avec le modèle mais ces variables pourraient ne pas être utilisées à l'avenir. "
   ]
  },
  {
   "cell_type": "code",
   "id": "b6d83626",
   "metadata": {},
   "source": [
    "# Imputing des colonnes avec les modèles entrainés\n",
    "for col in cols_na:\n",
    "    df_common.loc[df_common[col].isna(), col] = models[col][0].predict(df_common.loc[df_common[col].isna(), models[col][1]])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75da4a12",
   "metadata": {},
   "source": [
    "df_common.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9aaefb7e",
   "metadata": {},
   "source": [
    "On a maintenant uniquement des valeurs numériques et plus aucun nan dans les colonnes communes au 2 datasets. On va pouvoir commencer à faire l'analyse des colonnes spécifiques. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb664e",
   "metadata": {},
   "source": [
    "# Gestion du dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "id": "f34bbbf0",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "dataset_1_plot_path = \"../outputs/dataset_1/\"\n",
    "\"\"\"Chemin disque pour la sauvegarde des graphiques liés au dataset 1\"\"\"\n",
    "df_1_targets: list[str] = first_dataset_cols\n",
    "\"\"\"Propriétés cibles que l'on veut compléter dans cette analyse\"\"\"\n",
    "df_1_properties: list[str]  = df_common.columns.to_list()\n",
    "\n",
    "os.makedirs(dataset_1_plot_path, exist_ok=True)\n",
    "\n",
    "first_dataset_cols # colonnes spécifiques au premier dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_1_properties",
   "id": "214918ce81d7a7af",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7500c1d3",
   "metadata": {},
   "source": [
    "# Création du dataset avec df_common pour récupérer toutes les valeurs des éléments chimiques\n",
    "df_1 = pd.concat([df_common, df_first[first_dataset_cols]], axis=1)\n",
    "\"\"\"Dataset complet contenant les colonnes 'df_1_targets' et 'df_1_properties'\"\"\"\n",
    "df_1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8256f8d0",
   "metadata": {},
   "source": [
    "# Vérification de la cohérence de la taille du dataset concaténé\n",
    "print(f\"Taille du dataset 1         : {df_first.shape}\")\n",
    "print(f\"Taille du dataset commun    : {df_common.shape}\")\n",
    "print(f\"Taille du dataset 1 complet : {df_1.shape}\")\n",
    "assert df_1.shape[0] == df_first.shape[0]\n",
    "# Si l'assert bloque, alors la concaténation à rajouter des lignes, ce qui n'est pas souhaitable : on veut les colonnes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Recherche de corrélation (propriétés physiques)",
   "id": "b6d5254399d8f89c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Analyse de corrélations potientielles entre les propriétés mécaniques\n",
    "targets_corr = df_1[df_1_targets].corr()\n",
    "\n",
    "# Affichage de matrices de corrélation entre les différentes propriétés mécaniques\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(targets_corr,\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Corrélation: Propriétés mécaniques')\n",
    "plt.xlabel('Propriétés mécaniques')\n",
    "plt.ylabel('Propriétés mécaniques')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "2a927db2a2732342",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "On constate de fortes corrélation entre les propriétés mécaniques (deux à deux) :\n",
    "- UTS' et 'Yield_strength\n",
    "- Elongation et Reduction_area\n",
    "\n",
    "Cela correspond à de la résistance et de la flexibilité.\n",
    "On peut en apprendre plus en graphant leurs distributions"
   ],
   "id": "5bd19adf7f9185c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_1[df_1_targets].dropna(), bins=50, kde=True)\n",
    "plt.title(f'Répartition des propriétés mécaniques (non-na)')\n",
    "plt.xlabel(col)\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "8389d6f8d68cc5a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_1[['Elongation', 'Reduction_area']].dropna(), bins=50, kde=True)\n",
    "plt.title(f'Répartition des propriétés mécaniques (non-na)')\n",
    "plt.xlabel(col)\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "f9ba10e83ffef52d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_1[['UTS', 'Yield_strength']].dropna(), bins=50, kde=True)\n",
    "plt.title(f'Répartition des propriétés mécaniques (non-na)')\n",
    "plt.xlabel(col)\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "d4e549bedcdae6e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Quand on rapproche les distributions des propriétés physiques deux à deux, on se rend compte qu'il existe une nette ressemblance, même au niveau de l'échelle. Elles sont deux à deux des candidates idéales pour l'imputation, à condition qu'elles ne soient pas manquantes en même temps.",
   "id": "5b231ce431198d42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "uts_missing = df_1['UTS'].isna()\n",
    "yield_missing = df_1['Yield_strength'].isna()\n",
    "\n",
    "cases = {\n",
    "    'Both Missing'      : (uts_missing & yield_missing).sum(),\n",
    "    'Only UTS Missing'  : (uts_missing & ~yield_missing).sum(),\n",
    "    'Only Yield Missing': (~uts_missing & yield_missing).sum(),\n",
    "    'Both Present'      : (~uts_missing & ~yield_missing).sum()\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(cases.keys(), cases.values(), edgecolor='black', linewidth=1.2)\n",
    "plt.ylabel('Quantité', fontweight='bold')\n",
    "plt.title('Comparaison des valeurs manquantes', fontweight='bold', fontsize=13)\n",
    "plt.xticks(rotation=15)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "9d2398463c56b73a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Elongation_missing = df_1['Elongation'].isna()\n",
    "Reduction_area_missing = df_1['Reduction_area'].isna()\n",
    "\n",
    "cases = {\n",
    "    'Both Missing'               : (Elongation_missing & Reduction_area_missing).sum(),\n",
    "    'Only Elongation Missing'    : (Elongation_missing & ~Reduction_area_missing).sum(),\n",
    "    'Only Reduction_area Missing': (~Elongation_missing & Reduction_area_missing).sum(),\n",
    "    'Both Present'               : (~Elongation_missing & ~Reduction_area_missing).sum()\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(cases.keys(), cases.values(), edgecolor='black', linewidth=1.2)\n",
    "plt.ylabel('Quantité', fontweight='bold')\n",
    "plt.title('Comparaison des valeurs manquantes', fontweight='bold', fontsize=13)\n",
    "plt.xticks(rotation=15)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "aa45992aa0c092c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Malheureusement, la majorité des cas manquants sont présents sur les deux valeurs à la fois, limitant la possibilité d'utiliser notre plus forte corrélation pour cette tâche.",
   "id": "8dcde2798e577c20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Seconde vue de la corrélation des valeures manquantes pour les propriétés physiques (sous forme de heatmap)",
   "id": "6e3634d0750d4b69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "missing_correlation = df_1[df_1_targets].isnull().corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(missing_correlation, annot=True, cmap='coolwarm', center=0,\n",
    "            vmin=-1, vmax=1, square=True)\n",
    "plt.title('Corrélation entre les valeurs manquantes')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "cols_with_na = df_1.columns[df_1.isnull().any()].tolist()\n",
    "if len(cols_with_na) > 1:\n",
    "    for i, col1 in enumerate(cols_with_na):\n",
    "        for col2 in cols_with_na[i+1:]:\n",
    "            contingency = pd.crosstab(df_1[col1].isnull(),\n",
    "                                     df_1[col2].isnull(),\n",
    "                                     margins=True)"
   ],
   "id": "b82e78766aff66b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Correctif des données pour la colonne 'Interpass_temp'\n",
    "def replace_interval_mean(val):\n",
    "    if '-' in str(val):\n",
    "        bounds = val.split('-')\n",
    "        lower_bound = float(bounds[0])\n",
    "        upper_bound = float(bounds[1])\n",
    "        return (lower_bound + upper_bound) / 2\n",
    "    else:\n",
    "        return float(val)\n",
    "\n",
    "# df_1.loc[:, 'Interpass_temp'] = df_1['Interpass_temp'].apply(replace_interval_mean).astype(float)"
   ],
   "id": "4662b5370cbc2ed2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Recherche de nouvelles corrélations",
   "id": "fcaaa77291ae5b44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_1_targets_to_properties_correlation_matrix = df_1[df_1_targets + df_1_properties].corr()\n",
    "\"\"\"La matrice de corrélation entre les targets et properties du dataset 1\"\"\"\n",
    "top_n = 30"
   ],
   "id": "cbe48699e80c5454",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_top_correlations(corr_matrix, target_cols, property_cols, top_n=30):\n",
    "    \"\"\"\n",
    "    Extrait les top N corrélations entre targets et properties\n",
    "\n",
    "    \"\"\"\n",
    "    # Assurer que target_cols est une liste\n",
    "    if isinstance(target_cols, str):\n",
    "        target_cols = [target_cols]\n",
    "\n",
    "    corr_targets_properties = corr_matrix.loc[target_cols, property_cols]\n",
    "    corr_flat = corr_targets_properties.stack()\n",
    "    top_corr = corr_flat.abs().sort_values(ascending=False).head(top_n)\n",
    "    top_corr_values = corr_flat[top_corr.index]\n",
    "\n",
    "    return top_corr_values"
   ],
   "id": "99cc48c3e17a142f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_correlation_barh(corr_values, top_n, title=\"Corrélations entre Targets vs Properties\"):\n",
    "    \"\"\"\n",
    "        Plot un graphique horizontal des corrélations\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['red' if x < 0 else 'steelblue' for x in corr_values]\n",
    "    corr_values.plot(kind='barh', color=colors)\n",
    "    plt.title(f\"Top {top_n} {title}\")\n",
    "    plt.xlabel('Correlation')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "c7fbf5b762704808",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "corr_targets_properties = df_1_targets_to_properties_correlation_matrix.loc[df_1_targets, df_1_properties]\n",
    "\n",
    "corr_flat = corr_targets_properties.stack()\n",
    "top_corr = corr_flat.abs().sort_values(ascending=False).head(top_n)\n",
    "\n",
    "top_corr_values = corr_flat[top_corr.index]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red' if x < 0 else 'steelblue' for x in top_corr_values]\n",
    "top_corr_values.plot(kind='barh', color=colors)\n",
    "plt.title(f\"Top {top_n} des corrélations entre Targets vs Properties (dataset 1)\")\n",
    "plt.xlabel('Correlation')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Corrélations les plus fortes :\")\n",
    "print(top_corr_values)\n",
    "\"\"\""
   ],
   "id": "be909ecaa7005759",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "c = 'Yield_strength'\n",
    "top_corr_values = get_top_correlations(\n",
    "    df_1_targets_to_properties_correlation_matrix,\n",
    "    c,\n",
    "    df_1_properties,\n",
    "    top_n\n",
    ")\n",
    "plot_correlation_barh(top_corr_values, top_n, f\"corrélations entre {c} vs Properties (dataset 1)\")\n",
    "\n",
    "print(f\"Corrélations les plus fortes pour {c} :\")\n",
    "print(top_corr_values)"
   ],
   "id": "4e0edb94b7eff1da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "c = 'UTS'\n",
    "top_corr_values = get_top_correlations(\n",
    "    df_1_targets_to_properties_correlation_matrix,\n",
    "    c,\n",
    "    df_1_properties,\n",
    "    top_n\n",
    ")\n",
    "plot_correlation_barh(top_corr_values, top_n, f\"corrélations entre {c} vs Properties (dataset 1)\")\n",
    "\n",
    "print(f\"Corrélations les plus fortes pour {c} :\")\n",
    "print(top_corr_values)"
   ],
   "id": "f62247bfd6d2c98f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "c = 'Elongation'\n",
    "top_corr_values = get_top_correlations(\n",
    "    df_1_targets_to_properties_correlation_matrix,\n",
    "    c,\n",
    "    df_1_properties,\n",
    "    top_n\n",
    ")\n",
    "plot_correlation_barh(top_corr_values, top_n, f\"corrélations entre {c} vs Properties (dataset 1)\")\n",
    "\n",
    "print(f\"Corrélations les plus fortes pour {c} :\")\n",
    "print(top_corr_values)"
   ],
   "id": "f8e5e8e57d16bacc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "c = 'Reduction_area'\n",
    "top_corr_values = get_top_correlations(\n",
    "    df_1_targets_to_properties_correlation_matrix,\n",
    "    c,\n",
    "    df_1_properties,\n",
    "    top_n\n",
    ")\n",
    "plot_correlation_barh(top_corr_values, top_n, f\"corrélations entre {c} vs Properties (dataset 1)\")\n",
    "\n",
    "print(f\"Corrélations les plus fortes pour {c} :\")\n",
    "print(top_corr_values)"
   ],
   "id": "393c5a889dace2d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tests d'algorithmes de prédictions pour les valeurs mécaniques manquantes du dataset 1 à partir des propriétés de df_common",
   "id": "8e77c71cf7ac8e3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Grid search pour les regresseurs - recherche d'un regresseur optimal paramétré\n",
    "J'ai déjà exécuté la search une fois et retiré des paramètres des grid pour des raisons de performances"
   ],
   "id": "9166850011147926"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "def optimize_regressor(data, properties, targets, param_grid, model_class, model_name) -> dict:\n",
    "    \"\"\"\n",
    "    Optimise un modèle de régression pour chaque target via GridSearchCV\n",
    "    :param\n",
    "    data : pd.DataFrame - Dataset complet avec features et targets\n",
    "    properties : list -Noms des colonnes features\n",
    "    targets : list - Noms des colonnes targets\n",
    "    param_grid : dict - Grille de paramètres pour GridSearchCV\n",
    "    model_class : class -Classe du modèle (AdaBoostRegressor, XGBRegressor, etc.)\n",
    "    model_name : str - Nom du modèle pour affichage\n",
    "    :return\n",
    "        dict : Meilleurs modèles par target\n",
    "    \"\"\"\n",
    "    best_models = {}\n",
    "\n",
    "    for target in targets:\n",
    "        print(f\"\\nOptimisation {model_name} pour {target}...\")\n",
    "\n",
    "        train_mask = data[target].notna()\n",
    "        X_train = data.loc[train_mask, properties]\n",
    "        y_train = data.loc[train_mask, target]\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model_class(random_state=42),\n",
    "            param_grid=param_grid,\n",
    "            cv=5,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        print(f\"{target}: R² = {grid_search.best_score_:.3f} => {grid_search.best_params_}\")\n",
    "\n",
    "        best_models[target] = grid_search.best_estimator_\n",
    "\n",
    "    return best_models"
   ],
   "id": "4c293c85ba63b39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_optimized = df_1[df_1_properties + df_1_targets].copy()\n",
    "\"\"\"Le dataset utilisé pour entraîné les regresseurs puis où on imputeras les valeurs manquantes du dataset 1\"\"\""
   ],
   "id": "500577fa7d932203",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# AdaBoost",
   "id": "736547b9414f924c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ada_param_grid = {\n",
    "    'n_estimators': [75, 100, 150, 200],\n",
    "    'learning_rate': [0.1, 0.2, 0.5, 1.0],\n",
    "    'loss': ['square', 'exponential']\n",
    "}\n",
    "\n",
    "ada_best_models = optimize_regressor(\n",
    "    data=data_optimized,\n",
    "    properties=df_1_properties,\n",
    "    targets=df_1_targets,\n",
    "    param_grid=ada_param_grid,\n",
    "    model_class=AdaBoostRegressor,\n",
    "    model_name=\"AdaBoost\"\n",
    ")"
   ],
   "id": "e25de384e467153",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Optimisation AdaBoost pour Yield_strength...\n",
    "Yield_strength: R² = 0.413 => {'learning_rate': 1.0, 'loss': 'square', 'n_estimators': 200}\n",
    "\n",
    "Optimisation AdaBoost pour UTS...\n",
    "UTS: R² = 0.469 => {'learning_rate': 0.5, 'loss': 'square', 'n_estimators': 150}\n",
    "\n",
    "Optimisation AdaBoost pour Elongation...\n",
    "Elongation: R² = 0.296 => {'learning_rate': 0.5, 'loss': 'exponential', 'n_estimators': 100}\n",
    "\n",
    "Optimisation AdaBoost pour Reduction_area...\n",
    "Reduction_area: R² = 0.112 => {'learning_rate': 0.2, 'loss': 'exponential', 'n_estimators': 75}"
   ],
   "id": "aab97ff82c22faf5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# XGBoost\n",
    "Attention, le temps d'exécution de la cellule est long (style 20 minutes) ! J'ai réduit manuellement les paramètres pour trouver la solution optimal plus vite"
   ],
   "id": "cdd40f8d1a7dbb95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "xgb_param_grid = {\n",
    "    'n_estimators': [200, 500],\n",
    "    'max_depth': [3, 4],\n",
    "    'learning_rate': [0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "xgb_best_models = optimize_regressor(\n",
    "    data=data_optimized,\n",
    "    properties=df_1_properties,\n",
    "    targets=df_1_targets,\n",
    "    param_grid=xgb_param_grid,\n",
    "    model_class=xgb.XGBRegressor,\n",
    "    model_name=\"XGBoost\"\n",
    ")"
   ],
   "id": "369ac2d356979604",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Optimisation XGBoost pour Yield_strength...\n",
    "Yield_strength: R² = 0.591 => {'colsample_bytree': 0.7, 'learning_rate': 0.2, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 500, 'subsample': 1.0}\n",
    "\n",
    "Optimisation XGBoost pour UTS...\n",
    "UTS: R² = 0.566 => {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 500, 'subsample': 0.9}\n",
    "\n",
    "Optimisation XGBoost pour Elongation...\n",
    "Elongation: R² = 0.338 => {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 200, 'subsample': 1.0}\n",
    "\n",
    "Optimisation XGBoost pour Reduction_area...\n",
    "Reduction_area: R² = 0.341 => {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 300, 'subsample': 0.8}"
   ],
   "id": "98c2167ebab7724c"
  },
  {
   "cell_type": "code",
   "id": "e0998c06",
   "metadata": {},
   "source": [
    "# Imputation par XGBoost (meilleurs résultats)\n",
    "for target in df_1_targets:\n",
    "    missing_mask = data_optimized[target].isna()\n",
    "\n",
    "    if missing_mask.sum() > 0:\n",
    "        X_missing = data_optimized.loc[missing_mask, df_1_properties]\n",
    "        predictions = xgb_best_models[target].predict(X_missing)\n",
    "\n",
    "        data_optimized.loc[missing_mask, target] = predictions\n",
    "\n",
    "        print(f\"{target}: {missing_mask.sum()} valeurs imputées\")\n",
    "\n",
    "print(f\"Imputation terminée. Shape: {data_optimized.shape}\")\n",
    "print(f\"Valeurs manquantes restantes:\\n{data_optimized[df_1_targets].isna().sum()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# @TODO AJOUT DE NOISE A L'IMPUTATION BASE SUR UNE GAUSSIENNE\n",
    "# @TODO ANALYSE DES RESULTATS DE L'IMPUTATION"
   ],
   "id": "dcb0b9aaf2d9dd11",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "289e66de",
   "metadata": {},
   "source": [
    "# Gestion du dataset 2 "
   ]
  },
  {
   "cell_type": "code",
   "id": "f2bd04fc",
   "metadata": {},
   "source": [
    "second_dataset_cols # colonnes spécifiques au second dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb236297",
   "metadata": {},
   "source": [
    "df2 = pd.concat([df_common, df_second[second_dataset_cols]], axis=1)\n",
    "df2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f4aabb5a",
   "metadata": {},
   "source": [
    "df2.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5f22b023",
   "metadata": {},
   "source": [
    "print(f\"\\nValeurs manquantes par colonne:\")\n",
    "print(df2[second_dataset_cols].isna().sum())\n",
    "print(f\"\\nProportions manquantes:\")\n",
    "print((df2[second_dataset_cols].isna().sum() / len(df2) * 100).round(2))\n",
    "\n",
    "# Visualisation des valeurs manquantes\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "missing_counts = df2[second_dataset_cols].isna().sum()\n",
    "ax.bar(missing_counts.index, missing_counts.values, color=['red'])\n",
    "ax.set_ylabel('Nombre de valeurs manquantes')\n",
    "ax.set_title('Valeurs manquantes dans le dataset 2')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualisation du pattern de données manquantes\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df2[second_dataset_cols].isna(), cbar=True, cmap='viridis', yticklabels=False)\n",
    "plt.title('Pattern de valeurs manquantes - Dataset 2')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques descriptives des colonnes avec données non-manquantes\n",
    "print(f\"\\nStatistiques des colonnes du dataset 2:\")\n",
    "print(df2[second_dataset_cols].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c77e022d",
   "metadata": {},
   "source": [
    "On remarque que la température est en Celcius, pour éviter des problèmes liés au signe de la température on transforme ces températures en Kelvin (K) on va le faire à toutes les colonnes de températures\n",
    "On remarque aussi que quand charpy impact est présent charpy temp aussi ce qui va simplifier notre imputation"
   ]
  },
  {
   "cell_type": "code",
   "id": "47b99cc6",
   "metadata": {},
   "source": [
    "# Identifier toutes les colonnes de température\n",
    "temp_columns_in_df2 = [col for col in df2.columns if 'temp' in col.lower()]\n",
    "print(f\"\\nColonnes de température détectées: {temp_columns_in_df2}\")\n",
    "\n",
    "for temp_col in temp_columns_in_df2:\n",
    "    if temp_col in df2.columns:\n",
    "        n_before = df2[temp_col].isna().sum()\n",
    "        \n",
    "        # Conversion seulement pour les valeurs non-manquantes\n",
    "        mask = df2[temp_col].notna()\n",
    "        df2.loc[mask, temp_col] = df2.loc[mask, temp_col] + 273.15\n",
    "        \n",
    "        n_after = df2[temp_col].isna().sum()\n",
    "        \n",
    "        print(f\"\\n{temp_col}:\")\n",
    "        print(f\"Valeurs converties: {mask.sum()}\")\n",
    "        print(f\"Nouvelles statistiques:\")\n",
    "        print(df2[temp_col].describe())\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "61a07056",
   "metadata": {},
   "source": [
    "On va passer à l'imputation toujours par adaboost mais on sauvegarde avant l'imputation pour pouvoir comparer ensuite"
   ]
  },
  {
   "cell_type": "code",
   "id": "138472fc",
   "metadata": {},
   "source": [
    "# Sauvegarder les distributions AVANT imputation\n",
    "distributions_before = {}\n",
    "for col in second_dataset_cols:\n",
    "    distributions_before[col] = df2[col].dropna().copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2b55941",
   "metadata": {},
   "source": [
    "for col in second_dataset_cols:\n",
    "    if df2[col].notna().sum() > 0:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.histplot(df2[col].dropna(), bins=50, kde=True, color='steelblue')\n",
    "        plt.title(f'Distribution de {col} (valeurs non-manquantes: {df2[col].notna().sum()})')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Fréquence')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "## Imputation des colonnes du dataset 2 avec Adaboost\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "df2_full = df2.copy()\n",
    "\n",
    "imputation_models = {}\n",
    "\n",
    "for col in second_dataset_cols:\n",
    "    full = df2_full[list(set(df2_full.columns) - (set(second_dataset_cols) - {col}))].copy()\n",
    "    full.dropna(inplace=True)\n",
    "\n",
    "    cols = list(set(full.columns) - set(second_dataset_cols)) \n",
    "\n",
    "    X = full[cols]\n",
    "    y = full[col]\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    model = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        #score = model.score(X_test, y_test)\n",
    "        score = np.sqrt(np.mean((model.predict(X_test) - y_test) ** 2))\n",
    "        print(f\"Fold RMSE for {col}: {score}\")\n",
    "        print(f\"Fold R2 for {col}: {model.score(X_test, y_test):.2%}\\n\")\n",
    "    \n",
    "    print(\"_\"*40+\"\\n\")\n",
    "    model.fit(X, y)\n",
    "    models[col] = (model, cols)\n",
    "\n",
    "\n",
    "# Application de l'imputation pour les valeurs manquantes\n",
    "for col in second_dataset_cols:\n",
    "    if col in models:\n",
    "        n_missing = df2[col].isna().sum()\n",
    "        if n_missing > 0:\n",
    "            model, feature_cols = models[col]\n",
    "            indices_missing = df2.index[df2[col].isna()]\n",
    "            \n",
    "            X_missing = df2_full.loc[indices_missing, feature_cols]\n",
    "            predictions = model.predict(X_missing)\n",
    "            \n",
    "            df2.loc[indices_missing, col] = predictions\n",
    "            \n",
    "            print(f\"{col}: {n_missing} valeurs imputées\")\n",
    "        else:\n",
    "            print(f\"{col}: aucune valeur manquante\")\n",
    "    else:\n",
    "        print(f\"{col}: imputation non disponible (données insuffisantes)\")\n",
    "\n",
    "# Vérification finale\n",
    "print(f\"\\nVérification - Valeurs manquantes restantes:\")\n",
    "print(df2[second_dataset_cols].isna().sum())\n",
    "\n",
    "# Statistiques finales après imputation\n",
    "print(f\"\\nStatistiques du dataset 2 après imputation:\")\n",
    "print(df2[second_dataset_cols].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "82ee081d",
   "metadata": {},
   "source": [
    "Comparons les données avant et après l'imputation"
   ]
  },
  {
   "cell_type": "code",
   "id": "9535377c",
   "metadata": {},
   "source": [
    "# Comparaison AVANT/APRÈS imputation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "for idx, col in enumerate(second_dataset_cols):\n",
    "    ax_before = axes[idx, 0]\n",
    "    ax_before.hist(distributions_before[col], bins=40, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax_before.set_title(f'{col} - AVANT imputation (n={len(distributions_before[col])})', fontweight='bold')\n",
    "    ax_before.set_xlabel(col)\n",
    "    ax_before.set_ylabel('Fréquence')\n",
    "    ax_before.grid(True, alpha=0.3)\n",
    "    ax_before.axvline(distributions_before[col].mean(), color='red', linestyle='--', linewidth=2)\n",
    "    \n",
    "    ax_after = axes[idx, 1]\n",
    "    ax_after.hist(df2[col], bins=40, alpha=0.7, color='coral', edgecolor='black')\n",
    "    n_imputed = len(df2[col]) - len(distributions_before[col])\n",
    "    ax_after.set_title(f'{col} - APRÈS imputation (n={len(df2[col])}, {n_imputed} imputées)', fontweight='bold')\n",
    "    ax_after.set_xlabel(col)\n",
    "    ax_after.set_ylabel('Fréquence')\n",
    "    ax_after.grid(True, alpha=0.3)\n",
    "    ax_after.axvline(df2[col].mean(), color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for col in second_dataset_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  AVANT: n={len(distributions_before[col])}, mean={distributions_before[col].mean():.4f}, std={distributions_before[col].std():.4f}\")\n",
    "    print(f\"  APRÈS: n={len(df2[col])}, mean={df2[col].mean():.4f}, std={df2[col].std():.4f}\")\n",
    "    \n",
    "    delta_mean = df2[col].mean() - distributions_before[col].mean()\n",
    "    delta_std = df2[col].std() - distributions_before[col].std()\n",
    "    print(f\"  Changements: Δmean={delta_mean:+.4f}, Δstd={delta_std:+.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a505c6c",
   "metadata": {},
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Tester plusieurs valeurs de k pour KNN\n",
    "print(\"KNN Imputation - Test de différentes valeurs de k\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = df2_full.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "k_values = [3, 5, 7, 10]\n",
    "knn_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    df2_knn_temp = df2_full.copy()\n",
    "    df2_knn_scaled = df2_full.copy()\n",
    "    df2_knn_scaled[numeric_cols] = scaler.fit_transform(df2_full[numeric_cols])\n",
    "    \n",
    "    imputer_knn = KNNImputer(n_neighbors=k)\n",
    "    df2_knn_scaled[numeric_cols] = imputer_knn.fit_transform(df2_knn_scaled[numeric_cols])\n",
    "    df2_knn_temp[numeric_cols] = scaler.inverse_transform(df2_knn_scaled[numeric_cols])\n",
    "    \n",
    "    knn_results[k] = df2_knn_temp\n",
    "    print(f\"KNN (k={k}) - Valeurs manquantes restantes: {df2_knn_temp[second_dataset_cols].isna().sum().sum()}\")\n",
    "\n",
    "# Visualisation comparative: AVANT, AdaBoost, KNN(k=3,5,7,10)\n",
    "fig, axes = plt.subplots(2, 6, figsize=(24, 10))\n",
    "\n",
    "for idx, col in enumerate(second_dataset_cols):\n",
    "    # AVANT\n",
    "    axes[idx, 0].hist(distributions_before[col], bins=40, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    axes[idx, 0].set_title(f'{col} - AVANT', fontweight='bold', fontsize=10)\n",
    "    axes[idx, 0].set_xlabel(col, fontsize=8)\n",
    "    axes[idx, 0].set_ylabel('Fréquence', fontsize=8)\n",
    "    axes[idx, 0].grid(True, alpha=0.3)\n",
    "    axes[idx, 0].axvline(distributions_before[col].mean(), color='red', linestyle='--', linewidth=2)\n",
    "    axes[idx, 0].tick_params(labelsize=8)\n",
    "    \n",
    "    # AdaBoost\n",
    "    axes[idx, 1].hist(df2[col], bins=40, alpha=0.7, color='coral', edgecolor='black')\n",
    "    axes[idx, 1].set_title(f'{col} - AdaBoost', fontweight='bold', fontsize=10)\n",
    "    axes[idx, 1].set_xlabel(col, fontsize=8)\n",
    "    axes[idx, 1].set_ylabel('Fréquence', fontsize=8)\n",
    "    axes[idx, 1].grid(True, alpha=0.3)\n",
    "    axes[idx, 1].axvline(df2[col].mean(), color='red', linestyle='--', linewidth=2)\n",
    "    axes[idx, 1].tick_params(labelsize=8)\n",
    "    \n",
    "    # KNN avec différents k\n",
    "    for k_idx, k in enumerate(k_values):\n",
    "        axes[idx, 2+k_idx].hist(knn_results[k][col], bins=40, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        axes[idx, 2+k_idx].set_title(f'{col} - KNN(k={k})', fontweight='bold', fontsize=10)\n",
    "        axes[idx, 2+k_idx].set_xlabel(col, fontsize=8)\n",
    "        axes[idx, 2+k_idx].set_ylabel('Fréquence', fontsize=8)\n",
    "        axes[idx, 2+k_idx].grid(True, alpha=0.3)\n",
    "        axes[idx, 2+k_idx].axvline(knn_results[k][col].mean(), color='red', linestyle='--', linewidth=2)\n",
    "        axes[idx, 2+k_idx].tick_params(labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tableau comparatif détaillé\n",
    "print(\"\\nComparaison détaillée:\")\n",
    "for col in second_dataset_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  AVANT:\")\n",
    "    print(f\"    mean={distributions_before[col].mean():.4f}, std={distributions_before[col].std():.4f}\")\n",
    "    print(f\"  AdaBoost:\")\n",
    "    ab_mean = df2[col].mean()\n",
    "    ab_std = df2[col].std()\n",
    "    print(f\"    mean={ab_mean:.4f}, std={ab_std:.4f}\")\n",
    "    print(f\"    Δmean={ab_mean - distributions_before[col].mean():+.4f}, Δstd={ab_std - distributions_before[col].std():+.4f}\")\n",
    "    \n",
    "    for k in k_values:\n",
    "        knn_mean = knn_results[k][col].mean()\n",
    "        knn_std = knn_results[k][col].std()\n",
    "        print(f\"  KNN (k={k}):\")\n",
    "        print(f\"    mean={knn_mean:.4f}, std={knn_std:.4f}\")\n",
    "        print(f\"    Δmean={knn_mean - distributions_before[col].mean():+.4f}, Δstd={knn_std - distributions_before[col].std():+.4f}\")\n",
    "\n",
    "# Résumé des différences par rapport à AVANT\n",
    "print(\"\\n\\nRésumé des écarts (Δmean et Δstd):\")\n",
    "print(\"-\"*80)\n",
    "for col in second_dataset_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  {'Méthode':<15} {'Δmean':<15} {'Δstd':<15}\")\n",
    "    print(f\"  {'-'*45}\")\n",
    "    \n",
    "    ab_mean_diff = df2[col].mean() - distributions_before[col].mean()\n",
    "    ab_std_diff = df2[col].std() - distributions_before[col].std()\n",
    "    print(f\"  {'AdaBoost':<15} {ab_mean_diff:+.4f}        {ab_std_diff:+.4f}\")\n",
    "    \n",
    "    for k in k_values:\n",
    "        knn_mean_diff = knn_results[k][col].mean() - distributions_before[col].mean()\n",
    "        knn_std_diff = knn_results[k][col].std() - distributions_before[col].std()\n",
    "        print(f\"  {'KNN(k='+str(k)+')':<15} {knn_mean_diff:+.4f}        {knn_std_diff:+.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "567f73e3",
   "metadata": {},
   "source": [
    "# Merge des data"
   ]
  },
  {
   "cell_type": "code",
   "id": "2abb458e",
   "metadata": {},
   "source": [
    "# TODO"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "374de796",
   "metadata": {},
   "source": [
    "# Trouver un label de qualité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72bf3c3",
   "metadata": {},
   "source": [
    "PCA en 2D sur les colonnes qui peuvent se rapporter à la qualité pour faire une visualisation dans le même style que celle du cours avec la criminalité pour espérer avoir un label cohérent. "
   ]
  },
  {
   "cell_type": "code",
   "id": "455f0811",
   "metadata": {},
   "source": [
    "# TODO"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ab2b1d19",
   "metadata": {},
   "source": [
    "# Fit des modèles sur le label créé "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef3865",
   "metadata": {},
   "source": [
    "Tester différents modèles avec preprocessing adéquat..."
   ]
  },
  {
   "cell_type": "code",
   "id": "928951f2",
   "metadata": {},
   "source": [
    "# TODO"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
